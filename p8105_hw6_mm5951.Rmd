---
title: "Homework 6"
author: "mm5951"
date: "`r Sys.Date()`"
output: github_document
---

````{r, include = FALSE, message=FALSE, warning=FALSE}
library(tidyverse)
library(modelr)
library(corrplot)

knitr::opts_chunk$set(
	echo = TRUE,
	warning = FALSE,
  fig.width = 6,
  fig.asp = .6,
  out.width = "90%"
)
theme_set(theme_minimal() + theme(legend.position = "bottom"))
options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)
scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```

## Problem 1

Solutions provided by teaching team.


## Problem 2

### Data wrangling  

First, I import the dataset using `read_csv()` and rename empty observations to "na" ("", "Unknown"). Then, I wrangle data as per problem instructions. This includes:

* Create a new "city_state" variable (e.g. “Baltimore, MD”) with `mutate()` and order by alphabetical orden using `str_c()`.
* Create a new "resolution" variable using the `disposition` syntaxis, indicating whehter a case is solved (those for which the disposition is “Closed by arrest”).
* Ensure that relevant variables are numeric using `as.numeric()` within a `mutate()` syntaxis.
* Omit cities Dallas, TX; Phoenix, AZ; and Kansas City, MO, as these don’t report victim race using `filter()`.
* Note one entry "Tulsa, AL" is excluded using `filter()`, as it is unclear whether it refers to Tulsa, Oklahoma or Birmingham, Alabama (data entry mistake).
* Limit the analysis those for whom "victim_race" is white or black using `filter(df, %in%)`.

```{r, warning = FALSE, message = FALSE}
homicide_df = read_csv("./data/homicide-data.csv", na = c("","Unknown")) %>% 
  mutate(city_state = str_c(city, state, sep = ", "),
         resolution = as.numeric(disposition == "Closed by arrest"),
         victim_age = as.numeric(victim_age),
         victim_sex = as.numeric(victim_sex == "Female")
  ) %>% 
  relocate(city_state) %>% 
  filter(city_state != "Dallas, TX",
         city_state != "Phoenix, AZ",
         city_state != "Kansas City, MO",
         city_state != "Tulsa, AL",
         victim_race %in%  c("Black", "White")
        )
```

        
***** victim_race = as.numeric(victim_race == "White"),
         

### Logistic regression 

**PROMPT**: For the city of Baltimore, MD, use the glm function to fit a logistic regression with resolved vs unresolved as the outcome and victim age, sex and race as predictors. Save the output of glm as an R object; apply the broom::tidy to this object; and obtain the estimate and confidence interval of the adjusted odds ratio for solving homicides comparing male victims to female victims keeping all other variables fixed.

Now run glm for each of the cities in your dataset, and extract the adjusted odds ratio (and CI) for solving homicides comparing male victims to female victims. Do this within a “tidy” pipeline, making use of purrr::map, list columns, and unnest as necessary to create a dataframe with estimated ORs and CIs for each city.

### Logsitic Regression Analysis: Baltimore, MD

Firstly, for the city of Baltimore, MD, I use the `glm()` function to fit a logistic regression for the "resolution" as the outcome and "victim age, sex and race" as predictors. The output of this analysis is saved as an R object using `saveRDS()`.

```{r}
baltimore_df = homicide_df %>%
  filter(city == "Baltimore") %>%
  glm(resolution ~ victim_age + victim_race + victim_sex, data = ., family = binomial()) %>%
  saveRDS(file = "data/baltimore_glm.rds")
```

Next, I apply the `broom::tidy()` function to the "baltimore_glm" object to obtain the estimate as well as confidence interval (CI) of the adjusted odds ratio (OR) for solving homicides according to the victim's sex while keeping all other variables fixed (reference category: female).

```{r}
readRDS(file = "data/baltimore_glm.rds") %>%
  broom::tidy(conf.int = TRUE) %>% 
  janitor::clean_names() %>%
  mutate(OR = exp(estimate),
         conf_low = exp(conf_low),
         conf_high = exp(conf_high)) %>%
  select(term, OR, conf_low, conf_high) %>% 
  filter(term == "victim_sex") %>%
  knitr::kable(digits = 3)
```

### Logsitic Regression Analysis: all United States cities

Finally, I run a similar logistic regression model for all US cities (except for the abovementioned excluded due to data gaps). In doing so, the ìteration using `purrr::map()` is applied to do the process within a “tidy” pipeline. Finally, using `knitr::kable()`a table is generated containing each city's estimated ORs and CIs for solving homicides comparing male victims to female victims (ref: female).

```{r}
city_list = unique(homicide_df$city_state)
city_function = function(y){
  output = homicide_df %>%
    filter(city_state == y) %>%
    glm(resolution ~ victim_age + victim_race + victim_sex, data = ., family = binomial()) %>%
    broom::tidy(conf.int = TRUE) %>% 
    janitor::clean_names() %>%
    mutate(OR = exp(estimate),
         conf_low = exp(conf_low),
         conf_high = exp(conf_high)) %>%
    filter(term == "victim_sex") %>%
    select(term, OR, conf_low, conf_high)
}

final_list = map(city_list, city_function)
df_final = data.frame(city_list[1],final_list[[1]])
names(df_final)[1] = "city_state"
for (i in 2:47) {
  df = data.frame(city_list[i],final_list[[i]])
  names(df)[1] = "city_state"
  df_final = rbind(df_final, df)
}
knitr::kable(df_final, digits = 3)
```


### Data visualization

Finally, I create a plot that shows the estimated ORs and CIs for each city. The cities are organized according to estimated OR.

```{r}
OR_resolution = df_final %>%
  mutate(city_state = fct_reorder(city_state, OR)) %>% 
  ggplot(aes(x = city_state, y = OR)) +
  geom_point() +
  geom_errorbar(aes(ymin = conf_low, ymax = conf_high)) +
  labs(
    title = "OR and CI for a case resolution by victim's sex",
    x = "City, State",
    y = "Odds Ratio") +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1))

OR_resolution
```

In this graph's interpretation, it is important to note that the reference category is female. Any OR below 1 indicates that a female victim's case resolution (that is, to be close as "closed by arrest") is less likely when compared to a male's, all other variables hold constant. *For instance, as an example, looking into Denver, an OR = 2.087 (1.039, 4.297) indicates that a female victim's case has a 108.7% increase in the odds of being resolved when compared to males. Moreover, the CI does not contain the value one, which indicates it is significant. Conversely, Albuquerque's OR = 0.566 (0.266, 1.213) indicates a decrease in the case's odds to be resolved, although the CI contains 1 and therefore this result is not significant.* 

## Problem 3

### Data wrangling  

First, I load using `read_csv()` and wrangle it so it is tidy for regression analysis. Using `mutate()` certain variables are recoded so that binary data collection is more easily understandable (e.g. for `babysex` variable 1 refers to male and 2 to female), and using `sum(is.na())` I check for missing data on the dataset.

```{r, warning = FALSE, message = FALSE}
birthweight = read_csv("./data/birthweight.csv") %>% 
  mutate(babysex = factor(if_else(babysex == 1, "male", "female")),
         frace = factor(recode(frace, '1' = "White", '2' = "Black", '3' = "Asian", 
                               '4' = "Puerto Rican", '8' = "Other", '9' = "Unknown")),
         mrace = factor(recode(mrace,'1' = "White", '2' = "Black", '3' = "Asian", 
                               '4' = "Puerto Rican", '8' = "Other", '9' = "Unknown")),
         malform = factor(recode(malform, '0' = "absent", '1' = "present")))

sum(is.na(birthweight))
```


### Linear regression model for birth weight

Next, I generate a regression model for birth weight (`bwt`) against another potential associated variable. Since I suspect a baby's birth weight is typically associated with its length (`blenght`), I use a scatter plot as an exploratory analysis (using `ggplot()`).

```{r}
birthweight %>% 
  ggplot(aes(x = blength, y = bwt)) + 
  geom_point(alpha = .5) +
  labs(title = "Scatter plot of recorded baby's weight and length at birth",
        x = "Birth lenght (cm)", 
       y = "Birth weight (g)") +
  theme(plot.title = element_text(hjust = 0.5))
```

As per the visualization, it appears that there might be a linear relationship between a baby's length and weight at birth. Thus, in a linear logsitic regression model "fit1" I will input `blength` as a predictor of the birth weight `bwt` into the function `lm()`. To assess its fitting, I will plot the model residuals against fitted values.

```{r}
fit1 = lm(bwt ~ blength, data = birthweight)

fit1 %>% 
  broom::tidy() %>% 
  knitr::kable()

birthweight %>% 
  modelr::add_residuals(fit1) %>% 
  modelr::add_predictions(fit1) %>% 
  ggplot(aes(x = pred, y = resid)) +
  geom_point(alpha = .3) +
  labs(title = "Fit1 model: plot of model residuals against fitted values",
       x = "Fitted Values",
       y = "Residuals") +
  theme(plot.title = element_text(hjust = 0.5))
```

### Two alternative linear regression models for birth weight

In order to assess the fitting of the fit1 model, I will compare it to two alternative models, entailing:

* fit2: using length at birth and gestational age as predictors (main effects only);
* fit3: using head circumference, length, sex, and all interactions (including the three-way interaction) between these as predictors.

To do so, I use again the function `lm()` including the abovementioned variables in its arguments. Again, the model of residuals is plotted for both fit2 and fit3.

```{r}
fit2 = lm(bwt ~ blength + gaweeks,birthweight)

fit2 %>% 
  broom::tidy() %>% 
  knitr::kable()
birthweight %>% 
  modelr::add_residuals(fit2) %>% 
  modelr::add_predictions(fit2) %>% 
  ggplot(aes(x = pred, y = resid)) +
  geom_point(alpha = .3) +
  labs(title = "Fit2 model: plot of model residuals against fitted values",
       x = "Fitted Values",
       y = "Residuals") +
  theme(plot.title = element_text(hjust = 0.5))

fit3 = lm(bwt ~ bhead + blength + babysex + bhead*blength + bhead*babysex + blength*babysex + bhead*blength*babysex, birthweight)

fit3 %>% 
  broom::tidy() %>% 
  knitr::kable()
birthweight %>% 
  modelr::add_residuals(fit3) %>% 
  modelr::add_predictions(fit3) %>% 
  ggplot(aes(x = pred, y = resid)) +
  geom_point(alpha = .3) +
  labs(title = "Fit3 model: plot of model residuals against fitted values",
       x = "Fitted Values",
       y = "Residuals") +
  theme(plot.title = element_text(hjust = 0.5))
```

### Comparison of the fit1-3 linear regression models for birth weight

Next, I cross-validate models fit1-3 in order to compare the predictive performance of these competing methods. To do so, I will calculate and compare the root mean squared error (RMSE) calculated using the `modelr::crossv_mc` function.

```{r}
cv_df = 
  crossv_mc(birthweight, 100) %>% 
  mutate(
    train = map(train, as_tibble),
    test = map(test, as_tibble)
  ) %>% 
  mutate(
    fit1_mod = map(train, ~lm(bwt ~ blength, data = birthweight)),
    fit2_mod = map(train, ~lm(bwt ~ blength + gaweeks,birthweight)),
    fit3_mod = map(train, ~lm(bwt ~ bhead + blength + babysex + bhead*blength + bhead*babysex +       blength*babysex + bhead*blength*babysex, birthweight))
  ) %>% 
  mutate(
    rmse_fit1 = map2_dbl(fit1_mod, test, ~rmse(model = .x, data = .y)),
    rmse_fit2 = map2_dbl(fit2_mod, test, ~rmse(model = .x, data = .y)),
    rmse_fit3 = map2_dbl(fit3_mod, test, ~rmse(model = .x, data = .y))
  )
```

Next, I will compare plot and compare the distribution of RMSE values for each candidate model (fit1-3).

```{r}
cv_df %>% 
  select(starts_with("rmse")) %>% 
  pivot_longer(
    everything(),
    names_to = "model", 
    values_to = "rmse",
    names_prefix = "rmse_") %>% 
  mutate(model = fct_inorder(model)) %>% 
  ggplot(aes(x = model, y = rmse)) + geom_violin() +
  labs(
    title = "distribution of RMSE values for three candidate model",
    x = "Model",
    y = "Root-mean-square-deviation (RMSE)"
  ) +
  theme(plot.title = element_text(hjust = 0.5))
```

As per the visualization above, it becomes apparent that fit3 model has the lowest RMSE value (around 275) when compared to the other two models (RMSE around 325). Moreover, fit3 RMSE has a more centered distribution. Overall, it indicates that fit3 would be the best fitting model of the three candidates investigated.

*That's it for homework 6!*

